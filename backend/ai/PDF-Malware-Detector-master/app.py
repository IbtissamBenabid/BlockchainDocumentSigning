from flask import Flask, request, jsonify
import joblib
import pandas as pd
import numpy as np
import re
import os
from PyPDF2 import PdfReader
import tempfile
import psycopg2
from datetime import datetime
import uuid
import logging
from functools import wraps
from flask_cors import CORS

app = Flask(__name__)

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)
CORS(app, resources={r"/*": {"origins": "*"}})

# Load model and scaler
try:
    model = joblib.load('pdf_malware_model.pkl')
    scaler = joblib.load('scaler.pkl')
    logger.info("AI models loaded successfully")
except FileNotFoundError:
    logger.error("Error: pdf_malware_model.pkl or scaler.pkl not found. Run train_model.py to generate them.")
    exit(1)

# Database configuration
DB_CONFIG = {
    'host': os.getenv('DB_HOST', 'postgres'),
    'port': os.getenv('DB_PORT', '5432'),
    'database': os.getenv('DB_NAME', 'versafe'),
    'user': os.getenv('DB_USER', 'versafe_user'),
    'password': os.getenv('DB_PASSWORD', 'versafe_password')
}

# API Key for inter-service communication
INTERNAL_API_KEY = os.getenv('INTERNAL_API_KEY', 'your-internal-api-key')

def get_db_connection():
    """Get database connection."""
    try:
        conn = psycopg2.connect(**DB_CONFIG)
        return conn
    except Exception as e:
        logger.error(f"Database connection error: {e}")
        return None

def authenticate_api_key(f):
    """Decorator for API key authentication."""
    @wraps(f)
    def decorated_function(*args, **kwargs):
        api_key = request.headers.get('X-API-Key')
        if not api_key or api_key != INTERNAL_API_KEY:
            return jsonify({'error': 'Invalid or missing API key'}), 401
        return f(*args, **kwargs)
    return decorated_function
def extract_pdf_features(pdf_path):
    """Extract features from PDF using PyPDF2 with better error handling."""
    try:
        reader = PdfReader(pdf_path)
        
        metadata = reader.metadata
        if metadata is None:
            metadata = {}

        # Extract text safely from first page if available
        text_content = ""
        if len(reader.pages) > 0:
            try:
                text_content = reader.pages[0].extract_text() or ""
            except Exception as e:
                logger.warning(f"Failed to extract text from first page: {e}")

        features = {
            'pdfsize': os.path.getsize(pdf_path) / 1024,  # Size in KB
            'metadata size': len(str(metadata)),
            'pages': len(reader.pages),
            'xref Length': 0,  # PyPDF2 does not provide this easily
            'isEncrypted': 1 if reader.is_encrypted else 0,
            'embedded files': 0,  # Advanced parsing needed
            'images': 0,  # Advanced parsing needed
            'text': 'Yes' if text_content.strip() else 'No',
            'header': metadata.get('/Producer', '%PDF-1.4'),  # Common header approximation
            # Initialize all other features as 0 for now
            'obj': 0, 'endobj': 0, 'stream': 0, 'endstream': 0, 'xref': 0,
            'trailer': 0, 'startxref': 0, 'pageno': 0, 'encrypt': 0,
            'ObjStm': 0, 'Javascript': 0, 'AA': 0, 'OpenAction': 0,
            'Acroform': 0, 'JBIG2Decode': 0, 'RichMedia': 0, 'launch': 0,
            'EmbeddedFile': 0, 'XFA': 0, 'Colors': 0,
        }

        return features

    except Exception as e:
        logger.error(f"Error extracting features from {pdf_path}: {e}")
        return None
def fetch_and_map_pdf_metadata(document_id):
    """
    Fetch key-value metadata entries from the database for a given document_id
    and map them into a structured PDF metadata dictionary.
    """
   
    conn = get_db_connection()
    if not conn:
        return {'error': 'Database connection failed'}, 500

    cursor = conn.cursor()
    cursor.execute("SELECT key, value FROM document_metadata WHERE document_id = %s", (document_id,))
    
    rows = cursor.fetchall()  # get all rows
    
    cursor.close()
    conn.close()

    # Convert list of tuples (key, value) into a dict
    raw_metadata = {key: value for key, value in rows}

    mapped = {
        'pdfsize': round(float(raw_metadata.get('file_size', 0)) / 1024, 2),
        'metadata size': len(str(raw_metadata)),
        'pages': int(raw_metadata.get('pageCount', 0)),
        'xref Length': 0,
        'isEncrypted': 1 if raw_metadata.get('is_encrypted', 'false') in ('1', 'true', 'True') else 0,
        'embedded files': 0,
        'images': 0,
        'text': 'Yes' if raw_metadata.get('has_text', 'false') in ('1', 'true', 'True') else 'No',
        'header': raw_metadata.get('producer', '%PDF-1.4'),
        'wordCount':raw_metadata.get('wordCount', 0),
        'obj': 0, 'endobj': 0, 'stream': 0, 'endstream': 0, 'xref': 0,
        'trailer': 0, 'startxref': 0, 'pageno': 0, 'encrypt': 0,
        'ObjStm': 0, 'Javascript': 0, 'AA': 0, 'OpenAction': 0,
        'Acroform': 0, 'JBIG2Decode': 0, 'RichMedia': 0, 'launch': 0,
        'EmbeddedFile': 0, 'XFA': 0, 'Colors': 0,
    }

    return mapped

def preprocess_features(features_dict):
    """Preprocess features to match training data."""
    df = pd.DataFrame([features_dict])
    
    # Convert 'text' feature Yes/No to binary
    df['text'] = np.where(df['text'].str.contains('Yes'), 1, 0)
    # Convert header to binary feature
    df['header'] = df['header'].apply(lambda x: 1 if re.search(r'%PDF-\d*\.?\d*', x) else 0)
    df = df.fillna(0).replace([-1, -1.00], 0)
    
    expected_columns = ['pdfsize', 'metadata size', 'pages', 'xref Length', 'isEncrypted', 
                        'embedded files', 'images', 'text', 'header', 'obj', 'endobj', 
                        'stream', 'endstream', 'xref', 'trailer', 'startxref', 'pageno', 
                        'encrypt', 'ObjStm', 'Javascript', 'AA', 'OpenAction', 'Acroform', 
                        'JBIG2Decode', 'RichMedia', 'launch', 'EmbeddedFile', 'XFA', 'Colors']
    df = df.reindex(columns=expected_columns, fill_value=0)
    
    scaled_features = scaler.transform(df)
    return scaled_features

def store_analysis_result(document_id, result, confidence, features):
    """Store AI analysis result in database."""
    try:
        conn = get_db_connection()
        if not conn:
            return False

        cursor = conn.cursor()
        analysis_id = str(uuid.uuid4())

        cursor.execute("""
            INSERT INTO ai_analysis (
                id, document_id, analysis_type, result, confidence_score,
                features_extracted, model_version, analysis_timestamp
            ) VALUES (%s, %s, %s, %s, %s, %s, %s, %s)
        """, (
            analysis_id, document_id, 'MALWARE_SCAN', result, confidence,
            str(features), '1.0', datetime.now()
        ))

        conn.commit()
        cursor.close()
        conn.close()
        return True
    except Exception as e:
        logger.error(f"Failed to store analysis result: {e}")
        return False

@app.route('/health', methods=['GET'])
def health_check():
    return jsonify({
        'status': 'healthy',
        'service': 'ai-service',
        'version': '1.0',
        'model_loaded': model is not None,
        'timestamp': datetime.now().isoformat()
    })

@app.route('/api/ai/scan-pdf', methods=['POST'])
@authenticate_api_key
def scan_pdf():
    try:
        if 'file' not in request.files:
            return jsonify({'error': 'No file provided'}), 400

        file = request.files['file']
        if not file.filename.lower().endswith('.pdf'):
            return jsonify({'error': 'File must be a PDF'}), 400

        document_id = request.form.get('document_id')

        with tempfile.NamedTemporaryFile(delete=False, suffix='.pdf') as temp_file:
            file.save(temp_file.name)
            temp_path = temp_file.name

        features = extract_pdf_features(temp_path)
        os.unlink(temp_path)

        if features is None:
            return jsonify({'error': 'Failed to extract PDF features'}), 500

        scaled_features = preprocess_features(features)
        prediction = model.predict(scaled_features)[0]
        confidence = float(model.predict_proba(scaled_features)[0].max())
        result = 'Malicious' if prediction == 1 else 'Benign'

        if document_id:
            store_analysis_result(document_id, result, confidence, features)

        response = {
            'success': True,
            'result': result,
            'confidence': confidence,
            'risk_score': int(confidence * 100),
            'features_analyzed': len(features),
            'analysis_timestamp': datetime.now().isoformat(),
            'model_version': '1.0'
        }

        logger.info(f"PDF scan completed: {result} (confidence: {confidence:.3f})")
        return jsonify(response)

    except Exception as e:
        logger.error(f"PDF scan error: {e}")
        return jsonify({'error': str(e)}), 500

@app.route('/api/ai/analyze-document/<document_id>', methods=['POST'])
@authenticate_api_key
def analyze_document(document_id):
    try:
        conn = get_db_connection()
        if not conn:
            return jsonify({'error': 'Database connection failed'}), 500

        cursor = conn.cursor()
        print(document_id)
        cursor.execute("SELECT file_path FROM documents WHERE id = %s", (document_id,))
        result = cursor.fetchone()
        print(result)
        cursor.close()
        conn.close()

        if not result:
            return jsonify({'error': 'Document not found'}), 404

        file_path = result[0]
        print("file path",file_path)

        features = fetch_and_map_pdf_metadata(document_id)
        print(features)
        if features is None:
            return jsonify({'error': 'Failed to extract PDF features'}), 500

        scaled_features = preprocess_features(features)
        prediction = model.predict(scaled_features)[0]
        confidence = float(model.predict_proba(scaled_features)[0].max())
        result = 'Malicious' if prediction == 1 else 'Benign'

        store_analysis_result(document_id, result, confidence, features)

        response = {
            'success': True,
            'document_id': document_id,
            'result': result,
            'confidence': confidence,
            'risk_score': int(confidence * 100),
            'features': features,
            'analysis_timestamp': datetime.now().isoformat()
        }

        return jsonify(response)

    except Exception as e:
        logger.error(f"Document analysis error: {e}")
        return jsonify({'error': str(e)}), 500

@app.route('/api/ai/analysis-history/<document_id>', methods=['GET'])
@authenticate_api_key
def get_analysis_history(document_id):
    try:
        conn = get_db_connection()
        if not conn:
            return jsonify({'error': 'Database connection failed'}), 500

        cursor = conn.cursor()
        cursor.execute("""
            SELECT id, analysis_type, result, confidence_score,
                   model_version, analysis_timestamp
            FROM ai_analysis
            WHERE document_id = %s
            ORDER BY analysis_timestamp DESC
        """, (document_id,))

        results = cursor.fetchall()
        cursor.close()
        conn.close()

        history = []
        for row in results:
            history.append({
                'id': row[0],
                'analysis_type': row[1],
                'result': row[2],
                'confidence_score': float(row[3]) if row[3] else 0,
                'model_version': row[4],
                'analysis_timestamp': row[5].isoformat() if row[5] else None
            })

        return jsonify({
            'success': True,
            'document_id': document_id,
            'history': history
        })

    except Exception as e:
        logger.error(f"Analysis history error: {e}")
        return jsonify({'error': str(e)}), 500

if __name__ == '__main__':
    app.run(debug=True, host='0.0.0.0', port=8500)